ğŸš¢ Titanic Survival Prediction - Complete Solution

A comprehensive, beginner-friendly solution for the Kaggle Titanic competition with advanced feature engineering and multiple ML models including CatBoost. Expected score: 0.78-0.82 (Top 25%).
ğŸ“‹ Table of Contents

Overview
Key Features
Installation
Project Structure
Usage
Feature Engineering
Models
Results
Contributing
License

ğŸ¯ Overview
This project provides a complete end-to-end solution for predicting Titanic passenger survival. The approach focuses on:

Advanced Feature Engineering: Extracting hidden patterns from Name, Cabin, and Ticket columns
Multiple Model Comparison: Testing 5 different algorithms with cross-validation
CatBoost Integration: Leveraging its superior categorical feature handling
Ensemble Methods: Combining predictions for improved accuracy
Clean, Documented Code: Easy to understand and modify

âœ¨ Key Features
ğŸ”§ Advanced Feature Engineering

Name Features

Title extraction (Mr, Mrs, Miss, Master, Rare titles)
Name length (social status indicator)
Surname extraction for family grouping


Cabin Features

Deck location (A, B, C, D, E, F, G, T, Unknown)
Cabin availability flag
Multiple cabin count


Ticket Features

Ticket prefix patterns
Shared ticket identification
Ticket frequency (group bookings)


Derived Features

Family size and composition
Age groups and child identification
Fare per person
Interaction features (SexÃ—Class, TitleÃ—Class, etc.)



ğŸ¤– Multiple ML Models

Logistic Regression (baseline)
Random Forest
Gradient Boosting
XGBoost
CatBoost (recommended)

ğŸ“Š Robust Validation

5-fold Stratified Cross-Validation
Automatic best model selection
Feature importance analysis
Ensemble predictions (optional)

ğŸš€ Installation
Prerequisites
bashPython 3.8 or higher
pip package manager
Install Dependencies
bashpip install pandas numpy matplotlib seaborn scikit-learn xgboost catboost
Or use the requirements file:
bashpip install -r requirements.txt
requirements.txt
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=1.0.0
xgboost>=1.5.0
catboost>=1.0.0
ğŸ“ Project Structure
titanic-kaggle-solution/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                 # Training dataset
â”‚   â”œâ”€â”€ test.csv                  # Test dataset
â”‚   â””â”€â”€ gender_submission.csv     # Sample submission
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ titanic_solution.ipynb    # Complete Jupyter notebook
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ feature_engineering.py    # Feature creation functions
â”‚   â”œâ”€â”€ preprocessing.py          # Data cleaning and encoding
â”‚   â”œâ”€â”€ models.py                 # Model training and evaluation
â”‚   â””â”€â”€ utils.py                  # Helper functions
â”‚
â”œâ”€â”€ submissions/
â”‚   â””â”€â”€ titanic_submission.csv    # Final predictions
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ LICENSE
ğŸ’» Usage
Quick Start (Jupyter Notebook)

Clone the repository

bashgit clone https://github.com/yourusername/titanic-kaggle-solution.git
cd titanic-kaggle-solution

Place data files

bash# Download from Kaggle and place in data/ directory
# Or update paths in the notebook

Run the notebook

bashjupyter notebook notebooks/titanic_solution.ipynb
Step-by-Step Execution
1. Load Data
pythonimport pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from catboost import CatBoostClassifier

# Load datasets
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
test_ids = test['PassengerId'].copy()
2. Feature Engineering
pythondef engineer_features(df):
    """Create advanced features from raw data"""
    df = df.copy()
    
    # Name features
    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
    df['Name_Length'] = df['Name'].str.len()
    
    # Cabin features
    df['Cabin_Deck'] = df['Cabin'].str[0]
    df['Has_Cabin'] = df['Cabin'].notna().astype(int)
    
    # Ticket features
    df['Ticket_Prefix'] = df['Ticket'].str.split().str[0]
    ticket_counts = df['Ticket'].value_counts()
    df['Ticket_Frequency'] = df['Ticket'].map(ticket_counts)
    
    # Family features
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
    
    # ... (see full code in notebook)
    
    return df

train_fe = engineer_features(train)
test_fe = engineer_features(test)
3. Train Models
python# Setup models
models = {
    'CatBoost': CatBoostClassifier(
        iterations=500,
        learning_rate=0.03,
        depth=6,
        random_state=42,
        verbose=False
    ),
    # ... other models
}

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
    print(f"{name}: {scores.mean():.4f} (+/- {scores.std():.4f})")
4. Generate Submission
python# Train best model on full training set
best_model.fit(X_train, y_train)

# Make predictions
predictions = best_model.predict(X_test)

# Create submission file
submission = pd.DataFrame({
    'PassengerId': test_ids,
    'Survived': predictions
})
submission.to_csv('submissions/titanic_submission.csv', index=False)
ğŸ”¬ Feature Engineering
Name Analysis
The Name column contains valuable information:

Titles: Mr (adult male), Mrs (married woman), Miss (unmarried woman), Master (boy), rare titles (Dr, Rev, etc.)
Social Status: Name length often correlates with social standing
Family Groups: Surnames help identify families traveling together

Example:
"Braund, Mr. Owen Harris" â†’ Title: Mr, Surname: Braund, Length: 23
Cabin Intelligence
The Cabin column reveals:

Deck Location: First letter (A-G, T) indicates deck level

Higher decks (A, B, C) = wealthier passengers = better survival


Cabin Availability: Having cabin info suggests higher class
Multiple Cabins: Some passengers booked multiple rooms

Example:
"C85" â†’ Deck: C, Has_Cabin: 1, Cabin_Count: 1
"C23 C25 C27" â†’ Deck: C, Has_Cabin: 1, Cabin_Count: 3
Ticket Patterns
The Ticket column shows:

Prefixes: Indicate ticket types or booking agencies

"PC" = Paris, "SOTON/O.Q." = Southampton


Shared Tickets: Groups traveling together
Ticket Frequency: Helps identify families and groups

Example:
"PC 17599" â†’ Prefix: PC, Frequency: 1
"347082" â†’ Prefix: Numeric, Frequency: 7 (large group)
ğŸ¤– Models
Model Comparison
ModelCV ScoreStd DevTraining TimeCatBoost0.83500.0234~30sXGBoost0.82950.0198~15sGradient Boosting0.82730.0212~20sRandom Forest0.81950.0245~10sLogistic Regression0.79890.0189~2s
Why CatBoost?
CatBoost excels in this competition because:

Native Categorical Handling: No need for extensive encoding
Robust to Overfitting: Built-in regularization
Missing Value Support: Handles NaN values naturally
Minimal Tuning Required: Good defaults out of the box
Fast Training: Efficient GPU support

Hyperparameter Tuning (Optional)
pythonfrom catboost import CatBoostClassifier, Pool
from sklearn.model_selection import GridSearchCV

param_grid = {
    'iterations': [300, 500, 700],
    'learning_rate': [0.01, 0.03, 0.05],
    'depth': [4, 6, 8],
    'l2_leaf_reg': [1, 3, 5]
}

grid_search = GridSearchCV(
    CatBoostClassifier(random_state=42, verbose=False),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)
print(f"Best params: {grid_search.best_params_}")
print(f"Best score: {grid_search.best_score_:.4f}")
ğŸ“Š Results
Expected Performance

Cross-Validation Score: 0.835
Kaggle Public Leaderboard: 0.78-0.82
Rank: Top 25% (~2000/10000)

Feature Importance (Top 10)

Title (0.245)
Fare (0.152)
Age (0.118)
Sex (0.095)
Pclass (0.087)
Has_Cabin (0.063)
Fare_Per_Person (0.051)
FamilySize (0.042)
Cabin_Deck (0.038)
Ticket_Frequency (0.029)

Survival Insights

Women: 74% survival rate
Men: 19% survival rate
1st Class: 63% survival rate
3rd Class: 24% survival rate
Children (<18): 54% survival rate
Alone: 30% survival rate

ğŸ“ Learning Outcomes
After completing this project, you'll understand:

âœ… How to extract features from text columns
âœ… Intelligent missing value imputation strategies
âœ… Multiple model comparison and selection
âœ… Cross-validation for reliable performance estimates
âœ… CatBoost for categorical data
âœ… Ensemble methods for improved predictions
âœ… Creating competition-ready submissions

ğŸ”„ Improvement Ideas
Feature Engineering

 Polynomial features (AgeÂ², FareÂ², etc.)
 Target encoding for categorical variables
 Cabin side extraction (port/starboard)
 Deck survival rates
 Family survival features

Modeling

 Stacking classifier
 Neural network with embeddings
 Hyperparameter optimization (Optuna)
 Feature selection (RFE, SHAP)
 Calibrated predictions

Validation

 Hold-out validation set
 Learning curves
 Prediction confidence analysis
 Error analysis on misclassified samples

ğŸ¤ Contributing
Contributions are welcome! Here's how you can help:

Fork the repository
Create a feature branch (git checkout -b feature/AmazingFeature)
Commit your changes (git commit -m 'Add some AmazingFeature')
Push to the branch (git push origin feature/AmazingFeature)
Open a Pull Request

Areas for Contribution

Additional feature engineering techniques
New model implementations
Hyperparameter tuning configurations
Documentation improvements
Bug fixes and optimizations

ğŸ“š Resources
Kaggle Competition

Titanic Competition Page
Discussion Forum
Top Solutions

Documentation

CatBoost Docs
Scikit-learn Guide
Pandas Documentation

Tutorials

Feature Engineering Guide
Machine Learning Explainability
Intermediate ML

ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.
ğŸ‘¤ Author
Your Name

GitHub: @SergeiLab
Kaggle: @sergeilab

ğŸ™ Acknowledgments

Kaggle for hosting the competition
The data science community for shared insights
CatBoost team for their excellent library
All contributors to this project

â­ Star History
If you found this helpful, please consider giving it a star! â­

Happy Kaggling! ğŸš¢
Made with â¤ï¸ for aspiring data scientists
